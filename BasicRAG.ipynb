{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanuchaddha/Understanding-RAG/blob/main/BasicRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80e459c",
      "metadata": {
        "id": "b80e459c"
      },
      "source": [
        "# **Basic RAG (Retrieval-Augmented Generation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2366268",
      "metadata": {
        "id": "d2366268"
      },
      "source": [
        "### **Introduction:**\n",
        "Retrieval-Augmented Generation (RAG) combines information retrieval and text generation by leveraging pre-trained language models and vector-based search. In this example, we are using:\n",
        "- **FAISS** (Facebook AI Similarity Search) to index document embeddings and retrieve the most relevant ones.\n",
        "- **GPT-Neo** for text generation based on the query and retrieved documents.\n",
        "- **Gradio** to build a simple interactive UI to input a query and get a response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8115fd61",
      "metadata": {
        "id": "8115fd61"
      },
      "source": [
        "### **Step 1: Install Required Libraries**\n",
        "We need to install a few libraries to run the RAG model:\n",
        "- **transformers**: to load pre-trained models like `bert-base-uncased` and `gpt-neo`.\n",
        "- **faiss-cpu**: for efficient similarity search and document retrieval.\n",
        "- **gradio**: to create the interactive user interface.\n",
        "- **torch**: as it's needed to handle PyTorch models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5e614e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d5e614e",
        "outputId": "644a3c08-43bb-4a89-c3d1-6ed8cb310092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.0 (from gradio)\n",
            "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m580.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.0.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers faiss-cpu torch gradio langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff07b59",
      "metadata": {
        "id": "4ff07b59"
      },
      "source": [
        "### **Step 2: Import Necessary Libraries**\n",
        "In this step, we import the libraries needed for building the RAG system:\n",
        "- `AutoTokenizer` and `AutoModel` are used for handling BERT-based embeddings.\n",
        "- `faiss` is used to create an index for document embeddings and handle similarity searches.\n",
        "- `pipeline` helps load GPT-Neo for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2977b81e",
      "metadata": {
        "id": "2977b81e"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ea4e1a",
      "metadata": {
        "id": "12ea4e1a"
      },
      "source": [
        "### **Step 3: Loading Tokenizer and Model for Embeddings**\n",
        "Here, we load a pre-trained BERT model (`bert-base-uncased`) and its tokenizer. This model will be used to generate embeddings for our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb77b807",
      "metadata": {
        "id": "eb77b807"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model for embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5c8f6f",
      "metadata": {
        "id": "6b5c8f6f"
      },
      "source": [
        "### **Step 4: Define Documents**\n",
        "We define a list of documents that will be used to retrieve relevant information based on the user's query. These are simple sentences about AI and self-driving cars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8c69cf",
      "metadata": {
        "id": "be8c69cf"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"AI is revolutionizing self-driving technology.\",\n",
        "    \"Recent AI advancements focus on safety in autonomous vehicles.\",\n",
        "    \"AI is helping to reduce human errors in self-driving cars.\",\n",
        "    \"Large language models are driving advancements in machine learning.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e076f5",
      "metadata": {
        "id": "d5e076f5"
      },
      "source": [
        "### **Step 5: Function to Generate Embeddings**\n",
        "This function, `get_embeddings`, takes in a list of text strings, tokenizes them, and generates vector embeddings using the BERT model. These embeddings are what we will index using FAISS for similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f49d03",
      "metadata": {
        "id": "a7f49d03"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8362e83b",
      "metadata": {
        "id": "8362e83b"
      },
      "source": [
        "### **Step 6: Compute Embeddings for the Documents**\n",
        "We use the `get_embeddings` function to convert the documents into embeddings. Each document is represented as a vector in this step, which can later be indexed for similarity searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328e96c7",
      "metadata": {
        "id": "328e96c7"
      },
      "outputs": [],
      "source": [
        "# Compute embeddings for documents\n",
        "embeddings = get_embeddings(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec458e0a",
      "metadata": {
        "id": "ec458e0a"
      },
      "source": [
        "### **Step 7: Indexing the Embeddings using FAISS**\n",
        "FAISS is used to index the document embeddings. This allows us to efficiently search for documents that are most similar to the user's query. Here, we create a FAISS index and add the document embeddings to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773efb3e",
      "metadata": {
        "id": "773efb3e"
      },
      "outputs": [],
      "source": [
        "# Index the embeddings using FAISS\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739a2f7f",
      "metadata": {
        "id": "739a2f7f"
      },
      "source": [
        "### **Step 8: Load GPT-Neo for Text Generation**\n",
        "We load GPT-Neo (specifically `EleutherAI/gpt-neo-1.3B`), a model capable of generating human-like text. This will be used to generate a response based on the query and relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad42ae2",
      "metadata": {
        "id": "6ad42ae2"
      },
      "outputs": [],
      "source": [
        "# Load GPT-Neo for text generation\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e584b88",
      "metadata": {
        "id": "3e584b88"
      },
      "source": [
        "### **Step 9: Retrieval and Generation Function**\n",
        "The `retrieve_and_generate` function takes a user's query, generates its embedding, retrieves the top-k most relevant documents, and uses those documents to help GPT-Neo generate a response.\n",
        "- First, it calculates the embedding of the query and finds the nearest documents in the FAISS index.\n",
        "- Then, it combines the retrieved documents with the query to create a 'prompt' for GPT-Neo, which will generate the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "174433fc",
      "metadata": {
        "id": "174433fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve_and_generate(query, top_k=2):\n",
        "    query_embedding = get_embeddings([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
        "    prompt = query + \"\\n\\nRelevant information:\\n\" + \"\\n\".join(retrieved_docs)\n",
        "    generated_text = generator(prompt, max_length=100, do_sample=False)[0]['generated_text']\n",
        "    return generated_text\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f427bd",
      "metadata": {
        "id": "a5f427bd"
      },
      "source": [
        "### **Step 10: Gradio Interface Setup**\n",
        "We use **Gradio** to create a simple user interface where the user can input a query. The Gradio interface will call the `retrieve_and_generate` function to display the generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94300c20",
      "metadata": {
        "id": "94300c20"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=retrieve_and_generate,\n",
        "    inputs='text',\n",
        "    outputs='text',\n",
        "    title='Retrieval-Augmented Generation (RAG) Demo',\n",
        "    description='Ask a question related to AI and see RAG in action. It retrieves relevant information and generates a response using GPT-Neo.'\n",
        ")\n",
        "\n",
        "interface.launch()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7693f946",
      "metadata": {
        "id": "7693f946"
      },
      "source": [
        "### **Step 11: Run the Gradio Interface**\n",
        "Finally, we launch the Gradio app. This creates an interface where users can type a query and get a response generated by the RAG model.\n",
        "\n",
        "After running the code, you’ll get a link where you can interact with the model through a simple web UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d155f98",
      "metadata": {
        "id": "3d155f98"
      },
      "source": [
        "### **Example Query:**\n",
        "You can test the app with the following query:\n",
        "\n",
        "```plaintext\n",
        "Tell me about the latest AI trends in self-driving cars.\n",
        "```\n",
        "\n",
        "The RAG model will retrieve the most relevant documents and use them to generate a response using GPT-Neo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd04b5d",
      "metadata": {
        "id": "1fd04b5d"
      },
      "source": [
        "### **Conclusion:**\n",
        "This demonstrates a basic Retrieval-Augmented Generation (RAG) model with embeddings, FAISS for similarity search, GPT-Neo for text generation, and Gradio for UI interaction. This setup provides a foundation for building more advanced systems with additional documents, more complex retrieval mechanisms, or fine-tuned models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}